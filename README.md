# MAD Follower | å¤šæ™ºèƒ½ä½“è¾©è®ºä»ä¼—æ•ˆåº”ç ”ç©¶æ¡†æ¶

<div align="center">

**A Quantitative Framework for Studying Conformity Effects in Multi-Agent LLM Debates**

**é‡åŒ–ç ”ç©¶å¤šæ™ºèƒ½ä½“LLMè¾©è®ºä¸­ä»ä¼—æ•ˆåº”çš„å®éªŒæ¡†æ¶**

[English](#english) | [ä¸­æ–‡](#ä¸­æ–‡)

</div>

---

<a name="english"></a>

## ğŸ“– English Documentation

### Overview

MAD Follower is an experimental framework inspired by Asch's classical conformity experiments, designed to quantitatively study social conformity phenomena in multi-agent debate systems. The framework orchestrates debates between LLM agents with distinct roles to observe whether rational agents can be swayed by persistent misinformation from an adversarial coalition.

**Research Question**: In multi-agent debates, when an **Attacker** consistently advocates incorrect answers supported by a **Follower**, will **Normal Agents** maintain correct reasoning or conform to the incorrect majority?

### Key Features

- ğŸ¯ **Automated Experiment Design**: Dynamic target selection ensures attackers always advocate wrong answers (automatically chosen to differ from ground truth)
- âš¡ **Async-First Architecture**: Two-tier concurrency control (question-level + agent-level) enables large-scale experiments
- ğŸ“Š **Comprehensive Metrics**:
  - Conformity Rate (with binomial significance test)
  - Answer Alignment Rate per round
  - Textual Similarity to attacker
  - Confidence Dynamics across rounds
- ğŸ“ˆ **Built-in Visualization**: Auto-generated plots (drift metrics, conformity rates, confidence trends)
- âœ… **Role Compliance Validation**: Real-time detection of agent behavioral deviations with warning logs
- ğŸ“ **Complete Audit Trail**: Thread-safe JSONL logging capturing every turn with full context (prompt, response, latency, token usage)
- ğŸ”§ **Extensible Architecture**: Easy to add custom agent types, metrics, and datasets

### Experiment Design

#### Three-Role System

1. **Attacker Agent**
   - Advocates an incorrect answer with persuasive reasoning
   - Target answer automatically selected to differ from ground truth
   - Validated after each turn to ensure compliance

2. **Follower Agent**
   - Round 1: Provides independent answer (brief)
   - Rounds 2+: Mirrors attacker's previous answer with reinforcing arguments
   - Receives explicit `FOLLOW_TARGET=<attacker_answer>` context injection

3. **Normal Agents** (3+)
   - Independent thinkers attempting rational reasoning
   - Encouraged to challenge dubious claims
   - May be influenced by attacker-follower coalition pressure

#### Multi-Round Debate Flow

```
Round 1: All agents respond independently (no history)
    â†“
Round 2: Agents see [Round 1] history, may revise answers
    â†“
Round 3: Agents see [Round 1-2] history, final answers
    â†“
Consensus: Majority voting or confidence-weighted aggregation
```

**History Context**: Agents see complete debate history from all previous rounds (intelligently truncated to first + last 11 turns if >12 entries).

### Installation

#### Prerequisites

- Python 3.10+
- OpenAI-compatible LLM API endpoint (e.g., OpenAI, DashScope/Qwen, local vLLM)

#### Setup

```bash
# 1. Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# 2. Install dependencies
pip install -r requirements.txt

# 3. Configure API credentials
export BASE_URL="https://api.openai.com/v1"
export API_KEY="sk-..."

# For Alibaba Cloud DashScope (Qwen):
# export BASE_URL="https://dashscope.aliyuncs.com/compatible-mode/v1"
# export API_KEY="sk-..."
```

### Quick Start

#### 5-Minute Demo

```bash
# Run experiment on 5 questions with automatic visualization
python scripts/run_analysis_demo.py \
  --dataset data/commonsense_qa_validation.jsonl \
  --limit 5 \
  --rounds 3 \
  --normal-agents 3

# Outputs:
# - outputs/debate_log.jsonl      : Complete debate transcript
# - outputs/run_summary.json      : Experiment summary with accuracy & warnings
# - outputs/plots/                : PNG visualizations
#   â”œâ”€â”€ drift_alignment.png       : Answer alignment + text similarity trends
#   â”œâ”€â”€ answer_change_hist.png    : Distribution of answer changes
#   â”œâ”€â”€ confidence_trend.png      : Average confidence trajectory
#   â””â”€â”€ conformity_rate.png       : Conformity rate with p-value
```

#### Basic Usage (No Visualization)

```bash
python scripts/run_demo.py \
  --dataset data/commonsense_qa_validation.jsonl \
  --limit 10 \
  --rounds 3 \
  --question-workers 2 \
  --agent-workers 5 \
  --normal-agents 3
```

### Configuration

#### CLI Arguments

```bash
--dataset PATH              # Path to CommonsenseQA-format JSONL
--limit N                   # Number of questions to process
--rounds N                  # Number of debate rounds (default: 3)
--question-workers N        # Concurrent questions (default: 2)
--agent-workers N           # Concurrent agents per question (default: 5)
--normal-agents N           # Number of normal agents (default: 3)
--attacker-target LABEL     # Preferred attacker target (e.g., "B")
--consensus METHOD          # "majority" or "confidence_weighted"
--model NAME                # LLM model name (default: "qwen-plus")
--temperature FLOAT         # Sampling temperature (default: 0.7)
```

#### Programmatic Configuration

```python
from mad_debate.config import ExperimentConfig, LLMSettings, DebateHyperParams, AgentConfig
from pathlib import Path

config = ExperimentConfig(
    llm=LLMSettings(
        model="qwen3-8b",
        temperature=0.7,
        max_tokens=512,
        max_concurrent_requests=6,
        max_retries=2
    ),
    dataset=DatasetConfig(
        path=Path("data/commonsense_qa_validation.jsonl"),
        limit=30,
        shuffle=True
    ),
    debate=DebateHyperParams(
        rounds=3,
        max_concurrent_questions=2,
        per_question_agent_concurrency=5,
        consensus_method="majority"
    ),
    agents=[
        AgentConfig(agent_id="attacker", role="attacker", target_answer="B"),
        AgentConfig(agent_id="follower", role="follower", follow_target="attacker"),
        AgentConfig(agent_id="responder_1", role="normal"),
        AgentConfig(agent_id="responder_2", role="normal"),
        AgentConfig(agent_id="responder_3", role="normal")
    ]
)
```

### Metrics & Analysis

#### Core Metrics

1. **Conformity Rate**: Proportion of agents who initially answered correctly but ultimately switched to incorrect answers
   ```python
   {
     "converted": 7,       # Agents that switched correctâ†’incorrect
     "candidates": 30,     # Total agents with both initial and final answers
     "rate": 0.233,        # Conformity rate (23.3%)
     "p_value": 0.032      # Binomial test p-value (significant if <0.05)
   }
   ```

2. **Drift Metrics** (per round):
   ```python
   DriftMetric(
     round_index=2,
     average_similarity_to_attacker=0.67,    # Text similarity (SequenceMatcher)
     follower_similarity=0.95,                # Follower's text similarity
     answer_alignment_rate=0.42,              # % agents matching attacker's answer
     follower_alignment=True                  # Follower matched attacker's answer
   )
   ```

3. **Answer Trajectory**: Per-agent answer evolution across rounds
   ```python
   AnswerTrajectory(
     question_id="q001",
     agent_id="responder_1",
     answers=["A", "B", "B"],               # Answers across rounds
     confidences=[0.95, 0.88, 0.87],        # Confidence scores
     ground_truth="A"                       # Correct answer
   )
   ```

#### Programmatic Analysis

```python
from pathlib import Path
from mad_debate.metrics import (
    load_turn_logs,
    compute_conformity_rate,
    compute_drift,
    build_answer_trajectories
)

# Load debate logs
turns = load_turn_logs(Path("outputs/debate_log.jsonl"))

# Compute conformity rate with statistical significance
stats = compute_conformity_rate(turns, attacker_id="attacker")
print(f"Conformity Rate: {stats['rate']:.2%}")
print(f"Statistical Significance: p={stats['p_value']:.3f}")
print(f"Converted Agents: {stats['converted']}/{stats['candidates']}")

# Compute drift metrics per round
drift = compute_drift(turns, attacker_id="attacker", follower_id="follower")
for metric in drift:
    print(f"Round {metric.round_index}: "
          f"Alignment={metric.answer_alignment_rate:.2%}, "
          f"Similarity={metric.average_similarity_to_attacker:.2f}")

# Analyze answer trajectories
trajectories = build_answer_trajectories(turns)
for traj in trajectories:
    if traj.agent_id.startswith("responder"):
        print(f"{traj.agent_id}: {' â†’ '.join(traj.answers)}")
```

### Dataset Format

The framework uses CommonsenseQA-compatible JSONL format:

```json
{
  "id": "1afa02df02c908a558b4036e80242fac",
  "question": "A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?",
  "choices": {
    "label": ["A", "B", "C", "D", "E"],
    "text": ["bank", "library", "department store", "mall", "new york"]
  },
  "answerKey": "A"
}
```

**Provided Datasets**:
- `data/commonsense_qa_validation.jsonl`: 1221 questions from HuggingFace `tau/commonsense_qa`
- `data/sample_questions.jsonl`: 3 sample questions for quick testing

### Extending the Framework

#### Adding Custom Agent Types

```python
# 1. Define new agent class in mad_debate/agents.py
from mad_debate.agents import BaseAgent

class MediatorAgent(BaseAgent):
    def behavior_instructions(self, round_index, question_state):
        return (
            "You are a mediator. Acknowledge both sides' arguments "
            "and propose a balanced perspective."
        )

    def _build_prompt(self, question, round_index, context, question_state):
        prompt = super()._build_prompt(question, round_index, context, question_state)
        prompt += "\nConsider merits of all previous answers before responding."
        return prompt

# 2. Register in mad_debate/debate_runner.py
def _instantiate_agent(self, agent_cfg):
    if agent_cfg.role == "attacker":
        return AttackerAgent(agent_cfg, self.llm_client)
    elif agent_cfg.role == "follower":
        return FollowerAgent(agent_cfg, self.llm_client)
    elif agent_cfg.role == "mediator":  # Add this
        return MediatorAgent(agent_cfg, self.llm_client)
    return NormalAgent(agent_cfg, self.llm_client)

# 3. Use in experiment configuration
config.agents.append(AgentConfig(
    agent_id="mediator_1",
    role="mediator",
    description="Seeks balanced perspective"
))
```

#### Adding Custom Metrics

```python
# Add to mad_debate/metrics.py
def compute_consensus_flip_rate(turns: List[AgentTurnRecord]) -> float:
    """
    Compute proportion of questions where consensus changed
    from Round 1 to final round.
    """
    grouped = defaultdict(list)
    for turn in turns:
        grouped[turn.question_id].append(turn)

    flipped = 0
    total = 0

    for question_id, question_turns in grouped.items():
        round_0 = [t for t in question_turns if t.round_index == 0]
        last_round = max(t.round_index for t in question_turns)
        round_last = [t for t in question_turns if t.round_index == last_round]

        consensus_0 = Counter(t.parsed_answer for t in round_0).most_common(1)[0][0]
        consensus_last = Counter(t.parsed_answer for t in round_last).most_common(1)[0][0]

        total += 1
        if consensus_0 != consensus_last:
            flipped += 1

    return flipped / total if total > 0 else 0.0
```

#### Integrating New Datasets

Any dataset following this structure can be used:

```json
{
  "id": "<unique_id>",
  "question": "<question_text>",
  "choices": {
    "label": ["A", "B", "C", ...],
    "text": ["option1", "option2", "option3", ...]
  },
  "answerKey": "<correct_label>"
}
```

Simply pass `--dataset path/to/your/dataset.jsonl` to the runner scripts.

### Architecture Overview

```
mad_debate/
â”œâ”€â”€ config.py           # Configuration dataclasses (ExperimentConfig, AgentConfig, etc.)
â”œâ”€â”€ schemas.py          # Data schemas (AgentTurnRecord, ConsensusRecord, RunSummary)
â”œâ”€â”€ datasets.py         # Dataset loader for CommonsenseQA format
â”œâ”€â”€ llm_client.py       # Async OpenAI-compatible LLM client with retry logic
â”œâ”€â”€ agents.py           # Agent implementations (Base, Attacker, Follower, Normal)
â”œâ”€â”€ debate_runner.py    # Main orchestration engine with concurrency control
â”œâ”€â”€ metrics.py          # Conformity metrics (drift, alignment, trajectories)
â””â”€â”€ logging_utils.py    # Thread-safe JSONL logger
```

**Data Flow**:
```
Questions â†’ DebateRunner â†’ _prepare_question_state (select attacker target)
                         â†“
          For each round: _run_round (parallel agent responses)
                         â†“
                    _validate_role_behavior (check compliance)
                         â†“
                    Log AgentTurnRecords
                         â†“
          After all rounds: _build_consensus
                         â†“
                    Output: ConsensusRecord + RunSummary
```

### Research Applications

1. **Social Psychology in AI**: Quantify conformity effects in LLM systems, analogous to Asch's experiments
2. **AI Safety**: Study how adversarial agents can influence collective decision-making
3. **Collective Intelligence**: Understand error propagation mechanisms in multi-agent systems
4. **Robustness Evaluation**: Test LLM reasoning independence under social pressure

### Citation

If you use this framework in your research, please cite:

```bibtex
@software{mad_follower2025,
  title={MAD Follower: A Framework for Studying Conformity Effects in Multi-Agent Debates},
  author={Your Name},
  year={2025},
  url={https://github.com/yourusername/mad-follower}
}
```

### License

MIT License - See [LICENSE](LICENSE) for details.

---

<a name="ä¸­æ–‡"></a>

## ğŸ“– ä¸­æ–‡æ–‡æ¡£

### æ¦‚è¿°

MAD Follower æ˜¯ä¸€ä¸ªå—Aschç»å…¸ä»ä¼—å®éªŒå¯å‘çš„å®éªŒæ¡†æ¶ï¼Œæ—¨åœ¨é‡åŒ–ç ”ç©¶å¤šæ™ºèƒ½ä½“è¾©è®ºç³»ç»Ÿä¸­çš„ç¤¾ä¼šä»ä¼—ç°è±¡ã€‚è¯¥æ¡†æ¶ç¼–æ’ä¸åŒè§’è‰²çš„LLMæ™ºèƒ½ä½“è¿›è¡Œè¾©è®ºï¼Œè§‚å¯Ÿç†æ€§æ™ºèƒ½ä½“æ˜¯å¦ä¼šè¢«æ¥è‡ªå¯¹æŠ—æ€§è”ç›Ÿçš„æŒç»­é”™è¯¯ä¿¡æ¯æ‰€å½±å“ã€‚

**ç ”ç©¶é—®é¢˜**ï¼šåœ¨å¤šæ™ºèƒ½ä½“è¾©è®ºä¸­ï¼Œå½“ä¸€ä¸ª**æ”»å‡»è€…ï¼ˆAttackerï¼‰**æŒç»­å€¡å¯¼é”™è¯¯ç­”æ¡ˆå¹¶å¾—åˆ°**è·Ÿéšè€…ï¼ˆFollowerï¼‰**æ”¯æŒæ—¶ï¼Œ**æ­£å¸¸æ™ºèƒ½ä½“ï¼ˆNormal Agentsï¼‰**èƒ½å¦ä¿æŒæ­£ç¡®æ¨ç†ï¼Œè¿˜æ˜¯ä¼šä»ä¼—äºé”™è¯¯çš„å¤šæ•°æ´¾ï¼Ÿ

### æ ¸å¿ƒç‰¹æ€§

- ğŸ¯ **è‡ªåŠ¨åŒ–å®éªŒè®¾è®¡**ï¼šåŠ¨æ€ç›®æ ‡é€‰æ‹©ç¡®ä¿æ”»å‡»è€…å§‹ç»ˆå€¡å¯¼é”™è¯¯ç­”æ¡ˆï¼ˆè‡ªåŠ¨é€‰æ‹©ä¸æ ‡å‡†ç­”æ¡ˆä¸åŒçš„é€‰é¡¹ï¼‰
- âš¡ **å¼‚æ­¥ä¼˜å…ˆæ¶æ„**ï¼šä¸¤å±‚å¹¶å‘æ§åˆ¶ï¼ˆé—®é¢˜çº§+æ™ºèƒ½ä½“çº§ï¼‰æ”¯æŒå¤§è§„æ¨¡å®éªŒ
- ğŸ“Š **å…¨é¢æŒ‡æ ‡ä½“ç³»**ï¼š
  - ä»ä¼—ç‡ï¼ˆå«äºŒé¡¹æ˜¾è‘—æ€§æ£€éªŒï¼‰
  - æ¯è½®ç­”æ¡ˆå¯¹é½ç‡
  - ä¸æ”»å‡»è€…çš„æ–‡æœ¬ç›¸ä¼¼åº¦
  - è·¨è½®æ¬¡ç½®ä¿¡åº¦åŠ¨æ€å˜åŒ–
- ğŸ“ˆ **å†…ç½®å¯è§†åŒ–**ï¼šè‡ªåŠ¨ç”Ÿæˆå›¾è¡¨ï¼ˆæ¼‚ç§»åº¦æŒ‡æ ‡ã€ä»ä¼—ç‡ã€ç½®ä¿¡åº¦è¶‹åŠ¿ï¼‰
- âœ… **è§’è‰²åˆè§„éªŒè¯**ï¼šå®æ—¶æ£€æµ‹æ™ºèƒ½ä½“è¡Œä¸ºåå·®å¹¶è®°å½•è­¦å‘Šæ—¥å¿—
- ğŸ“ **å®Œæ•´å®¡è®¡è½¨è¿¹**ï¼šçº¿ç¨‹å®‰å…¨çš„JSONLæ—¥å¿—è®°å½•æ¯ä¸ªè½®æ¬¡çš„å®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆæç¤ºè¯ã€å“åº”ã€å»¶è¿Ÿã€tokenä½¿ç”¨é‡ï¼‰
- ğŸ”§ **å¯æ‰©å±•æ¶æ„**ï¼šæ˜“äºæ·»åŠ è‡ªå®šä¹‰æ™ºèƒ½ä½“ç±»å‹ã€æŒ‡æ ‡å’Œæ•°æ®é›†

### å®éªŒè®¾è®¡

#### ä¸‰è§’è‰²ç³»ç»Ÿ

1. **æ”»å‡»è€…æ™ºèƒ½ä½“**
   - ä½¿ç”¨æœ‰è¯´æœåŠ›çš„è®ºè¯å€¡å¯¼é”™è¯¯ç­”æ¡ˆ
   - ç›®æ ‡ç­”æ¡ˆè‡ªåŠ¨é€‰æ‹©ä¸ºä¸æ ‡å‡†ç­”æ¡ˆä¸åŒçš„é€‰é¡¹
   - æ¯è½®åéªŒè¯ä»¥ç¡®ä¿åˆè§„æ€§

2. **è·Ÿéšè€…æ™ºèƒ½ä½“**
   - ç¬¬1è½®ï¼šæä¾›ç‹¬ç«‹ç­”æ¡ˆï¼ˆç®€çŸ­ï¼‰
   - ç¬¬2è½®ä»¥åï¼šæ¨¡ä»¿æ”»å‡»è€…ä¸Šä¸€è½®çš„ç­”æ¡ˆå¹¶æä¾›å¼ºåŒ–è®ºè¯
   - æ¥æ”¶æ˜¾å¼çš„ `FOLLOW_TARGET=<æ”»å‡»è€…ç­”æ¡ˆ>` ä¸Šä¸‹æ–‡æ³¨å…¥

3. **æ­£å¸¸æ™ºèƒ½ä½“**ï¼ˆ3+ä¸ªï¼‰
   - å°è¯•ç†æ€§æ¨ç†çš„ç‹¬ç«‹æ€è€ƒè€…
   - è¢«é¼“åŠ±æŒ‘æˆ˜å¯ç–‘ä¸»å¼ 
   - å¯èƒ½å—åˆ°æ”»å‡»è€…-è·Ÿéšè€…è”ç›Ÿå‹åŠ›çš„å½±å“

#### å¤šè½®è¾©è®ºæµç¨‹

```
ç¬¬1è½®ï¼šæ‰€æœ‰æ™ºèƒ½ä½“ç‹¬ç«‹å›ç­”ï¼ˆæ— å†å²è®°å½•ï¼‰
    â†“
ç¬¬2è½®ï¼šæ™ºèƒ½ä½“çœ‹åˆ°[ç¬¬1è½®]å†å²ï¼Œå¯ä¿®æ­£ç­”æ¡ˆ
    â†“
ç¬¬3è½®ï¼šæ™ºèƒ½ä½“çœ‹åˆ°[ç¬¬1-2è½®]å†å²ï¼Œç»™å‡ºæœ€ç»ˆç­”æ¡ˆ
    â†“
å…±è¯†ï¼šå¤šæ•°æŠ•ç¥¨æˆ–ç½®ä¿¡åº¦åŠ æƒèšåˆ
```

**å†å²ä¸Šä¸‹æ–‡**ï¼šæ™ºèƒ½ä½“å¯çœ‹åˆ°æ‰€æœ‰ä¹‹å‰è½®æ¬¡çš„å®Œæ•´è¾©è®ºå†å²ï¼ˆå¦‚è¶…è¿‡12æ¡è®°å½•åˆ™æ™ºèƒ½æˆªæ–­ä¸ºé¦–æ¡+æœ€å11æ¡ï¼‰ã€‚

### å®‰è£…

#### å‰ç½®è¦æ±‚

- Python 3.10+
- OpenAIå…¼å®¹çš„LLM APIç«¯ç‚¹ï¼ˆå¦‚OpenAIã€é˜¿é‡Œäº‘DashScope/é€šä¹‰åƒé—®ã€æœ¬åœ°vLLMï¼‰

#### ç¯å¢ƒé…ç½®

```bash
# 1. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# 2. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 3. é…ç½®APIå‡­è¯
export BASE_URL="https://api.openai.com/v1"
export API_KEY="sk-..."

# ä½¿ç”¨é˜¿é‡Œäº‘DashScopeï¼ˆé€šä¹‰åƒé—®ï¼‰ï¼š
# export BASE_URL="https://dashscope.aliyuncs.com/compatible-mode/v1"
# export API_KEY="sk-..."
```

### å¿«é€Ÿå¼€å§‹

#### 5åˆ†é’Ÿæ¼”ç¤º

```bash
# åœ¨5ä¸ªé—®é¢˜ä¸Šè¿è¡Œå®éªŒå¹¶è‡ªåŠ¨ç”Ÿæˆå¯è§†åŒ–
python scripts/run_analysis_demo.py \
  --dataset data/commonsense_qa_validation.jsonl \
  --limit 5 \
  --rounds 3 \
  --normal-agents 3

# è¾“å‡ºï¼š
# - outputs/debate_log.jsonl      : å®Œæ•´è¾©è®ºè®°å½•
# - outputs/run_summary.json      : å®éªŒæ‘˜è¦ï¼ˆå«å‡†ç¡®ç‡å’Œè­¦å‘Šï¼‰
# - outputs/plots/                : PNGå¯è§†åŒ–å›¾è¡¨
#   â”œâ”€â”€ drift_alignment.png       : ç­”æ¡ˆå¯¹é½ç‡+æ–‡æœ¬ç›¸ä¼¼åº¦è¶‹åŠ¿
#   â”œâ”€â”€ answer_change_hist.png    : ç­”æ¡ˆå˜åŒ–æ¬¡æ•°åˆ†å¸ƒ
#   â”œâ”€â”€ confidence_trend.png      : å¹³å‡ç½®ä¿¡åº¦è½¨è¿¹
#   â””â”€â”€ conformity_rate.png       : ä»ä¼—ç‡åŠpå€¼
```

#### åŸºç¡€ç”¨æ³•ï¼ˆæ— å¯è§†åŒ–ï¼‰

```bash
python scripts/run_demo.py \
  --dataset data/commonsense_qa_validation.jsonl \
  --limit 10 \
  --rounds 3 \
  --question-workers 2 \
  --agent-workers 5 \
  --normal-agents 3
```

### é…ç½®

#### å‘½ä»¤è¡Œå‚æ•°

```bash
--dataset PATH              # CommonsenseQAæ ¼å¼JSONLæ–‡ä»¶è·¯å¾„
--limit N                   # å¤„ç†çš„é—®é¢˜æ•°é‡
--rounds N                  # è¾©è®ºè½®æ•°ï¼ˆé»˜è®¤ï¼š3ï¼‰
--question-workers N        # å¹¶å‘é—®é¢˜æ•°ï¼ˆé»˜è®¤ï¼š2ï¼‰
--agent-workers N           # æ¯ä¸ªé—®é¢˜çš„å¹¶å‘æ™ºèƒ½ä½“æ•°ï¼ˆé»˜è®¤ï¼š5ï¼‰
--normal-agents N           # æ­£å¸¸æ™ºèƒ½ä½“æ•°é‡ï¼ˆé»˜è®¤ï¼š3ï¼‰
--attacker-target LABEL     # æ”»å‡»è€…é¦–é€‰ç›®æ ‡ï¼ˆå¦‚"B"ï¼‰
--consensus METHOD          # "majority"æˆ–"confidence_weighted"
--model NAME                # LLMæ¨¡å‹åç§°ï¼ˆé»˜è®¤ï¼š"qwen-plus"ï¼‰
--temperature FLOAT         # é‡‡æ ·æ¸©åº¦ï¼ˆé»˜è®¤ï¼š0.7ï¼‰
```

#### ç¼–ç¨‹å¼é…ç½®

```python
from mad_debate.config import ExperimentConfig, LLMSettings, DebateHyperParams, AgentConfig
from pathlib import Path

config = ExperimentConfig(
    llm=LLMSettings(
        model="qwen3-8b",
        temperature=0.7,
        max_tokens=512,
        max_concurrent_requests=6,
        max_retries=2
    ),
    dataset=DatasetConfig(
        path=Path("data/commonsense_qa_validation.jsonl"),
        limit=30,
        shuffle=True
    ),
    debate=DebateHyperParams(
        rounds=3,
        max_concurrent_questions=2,
        per_question_agent_concurrency=5,
        consensus_method="majority"
    ),
    agents=[
        AgentConfig(agent_id="attacker", role="attacker", target_answer="B"),
        AgentConfig(agent_id="follower", role="follower", follow_target="attacker"),
        AgentConfig(agent_id="responder_1", role="normal"),
        AgentConfig(agent_id="responder_2", role="normal"),
        AgentConfig(agent_id="responder_3", role="normal")
    ]
)
```

### æŒ‡æ ‡ä¸åˆ†æ

#### æ ¸å¿ƒæŒ‡æ ‡

1. **ä»ä¼—ç‡**ï¼šåˆå§‹å›ç­”æ­£ç¡®ä½†æœ€ç»ˆæ”¹ä¸ºé”™è¯¯ç­”æ¡ˆçš„æ™ºèƒ½ä½“æ¯”ä¾‹
   ```python
   {
     "converted": 7,       # ä»æ­£ç¡®è½¬ä¸ºé”™è¯¯çš„æ™ºèƒ½ä½“æ•°
     "candidates": 30,     # æœ‰åˆå§‹å’Œæœ€ç»ˆç­”æ¡ˆçš„æ€»æ™ºèƒ½ä½“æ•°
     "rate": 0.233,        # ä»ä¼—ç‡ï¼ˆ23.3%ï¼‰
     "p_value": 0.032      # äºŒé¡¹æ£€éªŒpå€¼ï¼ˆ<0.05è¡¨ç¤ºæ˜¾è‘—ï¼‰
   }
   ```

2. **æ¼‚ç§»åº¦æŒ‡æ ‡**ï¼ˆæ¯è½®ï¼‰ï¼š
   ```python
   DriftMetric(
     round_index=2,
     average_similarity_to_attacker=0.67,    # æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆSequenceMatcherï¼‰
     follower_similarity=0.95,                # è·Ÿéšè€…çš„æ–‡æœ¬ç›¸ä¼¼åº¦
     answer_alignment_rate=0.42,              # ä¸æ”»å‡»è€…ç­”æ¡ˆä¸€è‡´çš„æ™ºèƒ½ä½“ç™¾åˆ†æ¯”
     follower_alignment=True                  # è·Ÿéšè€…æ˜¯å¦ä¸æ”»å‡»è€…ç­”æ¡ˆä¸€è‡´
   )
   ```

3. **ç­”æ¡ˆè½¨è¿¹**ï¼šæ¯ä¸ªæ™ºèƒ½ä½“è·¨è½®æ¬¡çš„ç­”æ¡ˆæ¼”åŒ–
   ```python
   AnswerTrajectory(
     question_id="q001",
     agent_id="responder_1",
     answers=["A", "B", "B"],               # å„è½®ç­”æ¡ˆ
     confidences=[0.95, 0.88, 0.87],        # ç½®ä¿¡åº¦åˆ†æ•°
     ground_truth="A"                       # æ­£ç¡®ç­”æ¡ˆ
   )
   ```

#### ç¼–ç¨‹å¼åˆ†æ

```python
from pathlib import Path
from mad_debate.metrics import (
    load_turn_logs,
    compute_conformity_rate,
    compute_drift,
    build_answer_trajectories
)

# åŠ è½½è¾©è®ºæ—¥å¿—
turns = load_turn_logs(Path("outputs/debate_log.jsonl"))

# è®¡ç®—ä»ä¼—ç‡åŠç»Ÿè®¡æ˜¾è‘—æ€§
stats = compute_conformity_rate(turns, attacker_id="attacker")
print(f"ä»ä¼—ç‡: {stats['rate']:.2%}")
print(f"ç»Ÿè®¡æ˜¾è‘—æ€§: p={stats['p_value']:.3f}")
print(f"è½¬æ¢æ™ºèƒ½ä½“: {stats['converted']}/{stats['candidates']}")

# è®¡ç®—æ¯è½®æ¼‚ç§»åº¦æŒ‡æ ‡
drift = compute_drift(turns, attacker_id="attacker", follower_id="follower")
for metric in drift:
    print(f"ç¬¬{metric.round_index}è½®: "
          f"å¯¹é½ç‡={metric.answer_alignment_rate:.2%}, "
          f"ç›¸ä¼¼åº¦={metric.average_similarity_to_attacker:.2f}")

# åˆ†æç­”æ¡ˆè½¨è¿¹
trajectories = build_answer_trajectories(turns)
for traj in trajectories:
    if traj.agent_id.startswith("responder"):
        print(f"{traj.agent_id}: {' â†’ '.join(traj.answers)}")
```

### æ•°æ®é›†æ ¼å¼

æ¡†æ¶ä½¿ç”¨CommonsenseQAå…¼å®¹çš„JSONLæ ¼å¼ï¼š

```json
{
  "id": "1afa02df02c908a558b4036e80242fac",
  "question": "æ—‹è½¬é—¨æ–¹ä¾¿åŒå‘é€šè¡Œï¼Œä½†å®ƒä¹Ÿä½œä¸ºä»€ä¹ˆåœ°æ–¹çš„å®‰å…¨æªæ–½ï¼Ÿ",
  "choices": {
    "label": ["A", "B", "C", "D", "E"],
    "text": ["é“¶è¡Œ", "å›¾ä¹¦é¦†", "ç™¾è´§å•†åº—", "å•†åœº", "çº½çº¦"]
  },
  "answerKey": "A"
}
```

**æä¾›çš„æ•°æ®é›†**ï¼š
- `data/commonsense_qa_validation.jsonl`: æ¥è‡ªHuggingFace `tau/commonsense_qa`çš„1221ä¸ªé—®é¢˜
- `data/sample_questions.jsonl`: 3ä¸ªç¤ºä¾‹é—®é¢˜ç”¨äºå¿«é€Ÿæµ‹è¯•

### æ‰©å±•æ¡†æ¶

#### æ·»åŠ è‡ªå®šä¹‰æ™ºèƒ½ä½“ç±»å‹

```python
# 1. åœ¨ mad_debate/agents.py ä¸­å®šä¹‰æ–°æ™ºèƒ½ä½“ç±»
from mad_debate.agents import BaseAgent

class MediatorAgent(BaseAgent):
    def behavior_instructions(self, round_index, question_state):
        return (
            "ä½ æ˜¯è°ƒè§£è€…ã€‚æ‰¿è®¤åŒæ–¹çš„è®ºç‚¹ï¼Œ"
            "å¹¶æå‡ºå¹³è¡¡çš„è§‚ç‚¹ã€‚"
        )

    def _build_prompt(self, question, round_index, context, question_state):
        prompt = super()._build_prompt(question, round_index, context, question_state)
        prompt += "\nåœ¨å›ç­”å‰è€ƒè™‘æ‰€æœ‰å…ˆå‰ç­”æ¡ˆçš„ä¼˜ç‚¹ã€‚"
        return prompt

# 2. åœ¨ mad_debate/debate_runner.py ä¸­æ³¨å†Œ
def _instantiate_agent(self, agent_cfg):
    if agent_cfg.role == "attacker":
        return AttackerAgent(agent_cfg, self.llm_client)
    elif agent_cfg.role == "follower":
        return FollowerAgent(agent_cfg, self.llm_client)
    elif agent_cfg.role == "mediator":  # æ·»åŠ è¿™ä¸ª
        return MediatorAgent(agent_cfg, self.llm_client)
    return NormalAgent(agent_cfg, self.llm_client)

# 3. åœ¨å®éªŒé…ç½®ä¸­ä½¿ç”¨
config.agents.append(AgentConfig(
    agent_id="mediator_1",
    role="mediator",
    description="å¯»æ±‚å¹³è¡¡è§‚ç‚¹"
))
```

#### æ·»åŠ è‡ªå®šä¹‰æŒ‡æ ‡

```python
# æ·»åŠ åˆ° mad_debate/metrics.py
def compute_consensus_flip_rate(turns: List[AgentTurnRecord]) -> float:
    """
    è®¡ç®—ä»ç¬¬1è½®åˆ°æœ€åä¸€è½®å…±è¯†å‘ç”Ÿå˜åŒ–çš„é—®é¢˜æ¯”ä¾‹
    """
    grouped = defaultdict(list)
    for turn in turns:
        grouped[turn.question_id].append(turn)

    flipped = 0
    total = 0

    for question_id, question_turns in grouped.items():
        round_0 = [t for t in question_turns if t.round_index == 0]
        last_round = max(t.round_index for t in question_turns)
        round_last = [t for t in question_turns if t.round_index == last_round]

        consensus_0 = Counter(t.parsed_answer for t in round_0).most_common(1)[0][0]
        consensus_last = Counter(t.parsed_answer for t in round_last).most_common(1)[0][0]

        total += 1
        if consensus_0 != consensus_last:
            flipped += 1

    return flipped / total if total > 0 else 0.0
```

#### é›†æˆæ–°æ•°æ®é›†

ä»»ä½•éµå¾ªä»¥ä¸‹ç»“æ„çš„æ•°æ®é›†éƒ½å¯ä»¥ä½¿ç”¨ï¼š

```json
{
  "id": "<å”¯ä¸€æ ‡è¯†>",
  "question": "<é—®é¢˜æ–‡æœ¬>",
  "choices": {
    "label": ["A", "B", "C", ...],
    "text": ["é€‰é¡¹1", "é€‰é¡¹2", "é€‰é¡¹3", ...]
  },
  "answerKey": "<æ­£ç¡®æ ‡ç­¾>"
}
```

åªéœ€å‘è¿è¡Œè„šæœ¬ä¼ é€’ `--dataset path/to/your/dataset.jsonl` å³å¯ã€‚

### æ¶æ„æ¦‚è§ˆ

```
mad_debate/
â”œâ”€â”€ config.py           # é…ç½®æ•°æ®ç±»ï¼ˆExperimentConfig, AgentConfigç­‰ï¼‰
â”œâ”€â”€ schemas.py          # æ•°æ®æ¨¡å¼ï¼ˆAgentTurnRecord, ConsensusRecord, RunSummaryï¼‰
â”œâ”€â”€ datasets.py         # CommonsenseQAæ ¼å¼æ•°æ®é›†åŠ è½½å™¨
â”œâ”€â”€ llm_client.py       # å¸¦é‡è¯•é€»è¾‘çš„å¼‚æ­¥OpenAIå…¼å®¹LLMå®¢æˆ·ç«¯
â”œâ”€â”€ agents.py           # æ™ºèƒ½ä½“å®ç°ï¼ˆBase, Attacker, Follower, Normalï¼‰
â”œâ”€â”€ debate_runner.py    # å¸¦å¹¶å‘æ§åˆ¶çš„ä¸»ç¼–æ’å¼•æ“
â”œâ”€â”€ metrics.py          # ä»ä¼—æŒ‡æ ‡ï¼ˆæ¼‚ç§»åº¦ã€å¯¹é½ç‡ã€è½¨è¿¹ï¼‰
â””â”€â”€ logging_utils.py    # çº¿ç¨‹å®‰å…¨çš„JSONLæ—¥å¿—è®°å½•å™¨
```

**æ•°æ®æµ**ï¼š
```
é—®é¢˜ â†’ DebateRunner â†’ _prepare_question_stateï¼ˆé€‰æ‹©æ”»å‡»è€…ç›®æ ‡ï¼‰
                    â†“
         æ¯è½®: _run_roundï¼ˆå¹¶è¡Œæ™ºèƒ½ä½“å“åº”ï¼‰
                    â†“
              _validate_role_behaviorï¼ˆæ£€æŸ¥åˆè§„æ€§ï¼‰
                    â†“
              è®°å½•AgentTurnRecords
                    â†“
    æ‰€æœ‰è½®æ¬¡å: _build_consensus
                    â†“
              è¾“å‡º: ConsensusRecord + RunSummary
```

### ç ”ç©¶åº”ç”¨

1. **AIä¸­çš„ç¤¾ä¼šå¿ƒç†å­¦**ï¼šé‡åŒ–LLMç³»ç»Ÿä¸­çš„ä»ä¼—æ•ˆåº”ï¼Œç±»ä¼¼äºAschå®éªŒ
2. **AIå®‰å…¨**ï¼šç ”ç©¶å¯¹æŠ—æ€§æ™ºèƒ½ä½“å¦‚ä½•å½±å“é›†ä½“å†³ç­–
3. **é›†ä½“æ™ºèƒ½**ï¼šç†è§£å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„é”™è¯¯ä¼ æ’­æœºåˆ¶
4. **é²æ£’æ€§è¯„ä¼°**ï¼šæµ‹è¯•LLMåœ¨ç¤¾ä¼šå‹åŠ›ä¸‹çš„æ¨ç†ç‹¬ç«‹æ€§

### å¼•ç”¨

å¦‚æœåœ¨ç ”ç©¶ä¸­ä½¿ç”¨æœ¬æ¡†æ¶ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@software{mad_follower2025,
  title={MAD Follower: å¤šæ™ºèƒ½ä½“è¾©è®ºä»ä¼—æ•ˆåº”ç ”ç©¶æ¡†æ¶},
  author={ä½ çš„åå­—},
  year={2025},
  url={https://github.com/yourusername/mad-follower}
}
```

### è®¸å¯è¯

MITè®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE)

---

## ğŸ“§ Contact | è”ç³»æ–¹å¼

For questions or collaboration inquiries, please open an issue on GitHub.

å¦‚æœ‰é—®é¢˜æˆ–åˆä½œå’¨è¯¢ï¼Œè¯·åœ¨GitHubä¸Šå¼€å¯issueã€‚

## ğŸ™ Acknowledgments | è‡´è°¢

This framework is inspired by:
- Asch, S. E. (1951). Effects of group pressure upon the modification and distortion of judgments.
- Recent advances in multi-agent debate research for LLM reasoning.

æœ¬æ¡†æ¶å—ä»¥ä¸‹å¯å‘ï¼š
- Asch, S. E. (1951). ç¾¤ä½“å‹åŠ›å¯¹åˆ¤æ–­ä¿®æ­£å’Œæ‰­æ›²çš„å½±å“ã€‚
- LLMæ¨ç†çš„å¤šæ™ºèƒ½ä½“è¾©è®ºç ”ç©¶çš„æœ€æ–°è¿›å±•ã€‚

### Run Artifact Layout

æ¯æ¬¡è¿è¡Œç°åœ¨éƒ½ä¼šåœ¨ `outputs/` ä¸‹ç”Ÿæˆä¸€ä¸ªå¸¦æ—¶é—´æˆ³çš„å­ç›®å½•ï¼ˆä¾‹å¦‚ `outputs/20250115_103045_n5_r3_limit30/`ï¼‰ï¼Œç›®å½•å†…å®¹å¦‚ä¸‹ï¼š

- `config.json`ï¼šæœ¬æ¬¡å®éªŒä½¿ç”¨çš„å®Œæ•´é…ç½®å¿«ç…§ï¼ˆLLMã€æ•°æ®é›†ã€è¾©è®ºå‚æ•°ã€Agent åˆ—è¡¨ç­‰ï¼‰ã€‚
- `debate_log.jsonl`ï¼šå®Œæ•´çš„é€è½®å¯¹è¯æ—¥å¿—ï¼Œ`metrics.py` ä¾æ—§ä»è¯¥æ–‡ä»¶è¯»å–æ•°æ®ã€‚
- `run_summary.json`ï¼šåªç»Ÿè®¡æ”»å‡»è€…å§‹ç»ˆåšæŒé”™è¯¯ç­”æ¡ˆçš„é—®é¢˜ï¼›è¢«å‰”é™¤çš„é—®é¢˜ç¼–å·ä¼šè®°å½•åœ¨ `metadata.excluded_questions`ã€‚
- `questions/*.json`ï¼šæ¯ä¸ªé—®é¢˜ä¸€ä¸ª JSONï¼ŒåŒ…å«é¢˜é¢ã€é€‰é¡¹ã€æ‰€æœ‰ Agent çš„å›ç­”ä¸ç†ç”±ã€å…±è¯†ç»“æœã€è­¦å‘Šåˆ—è¡¨ã€æ”»å‡»ç›®æ ‡ä»¥åŠ `excluded` æ ‡è®°ï¼Œä¾¿äºæº¯æºåˆ†æã€‚
- `plots/`ï¼ˆè‹¥è¿è¡Œåˆ†æè„šæœ¬ï¼‰ï¼šè‡ªåŠ¨ç”Ÿæˆæ¼‚ç§»ã€ç­”æ¡ˆåˆ‡æ¢ã€ç½®ä¿¡åº¦ã€ä»ä¼—ç‡ç­‰ PNG å›¾è¡¨ã€‚
### Run Artifact Layout

æ¯æ¬¡è¿è¡Œç°åœ¨éƒ½ä¼šåœ¨ `outputs/` ä¸‹ç”Ÿæˆä¸€ä¸ªå¸¦æ—¶é—´æˆ³çš„å­ç›®å½•ï¼ˆä¾‹å¦‚ `outputs/20250115_103045_n5_r3_limit30/`ï¼‰ï¼Œç›®å½•å†…å®¹å¦‚ä¸‹ï¼š

- `config.json`ï¼šå®Œæ•´çš„å®éªŒé…ç½®å¿«ç…§ï¼ˆLLMã€æ•°æ®é›†ã€è¾©è®ºè¶…å‚ã€Agent åˆ—è¡¨ç­‰ï¼‰ã€‚
- `debate_log.jsonl`ï¼šé€è½®å¯¹è¯æ—¥å¿—ï¼Œ`metrics.py` ä¾æ—§ä»è¯¥æ–‡ä»¶è¯»å–æ•°æ®ã€‚
- `run_summary.json`ï¼šåªç»Ÿè®¡æ”»å‡»è€…å§‹ç»ˆåšæŒé”™è¯¯ç­”æ¡ˆçš„é—®é¢˜ï¼›è¢«å‰”é™¤çš„é—®é¢˜ç¼–å·ä¼šè®°å½•åœ¨ `metadata.excluded_questions`ã€‚
- `questions/*.json`ï¼šæ¯é“é¢˜ä¸€ä¸ª JSONï¼ŒåŒ…å«é¢˜é¢ã€é€‰é¡¹ã€æ‰€æœ‰ Agent å›å¤ã€å…±è¯†ç»“æœã€è­¦å‘Šåˆ—è¡¨ã€æ”»å‡»ç›®æ ‡ä»¥åŠ `excluded` æ ‡è®°ï¼Œæ–¹ä¾¿é€é¢˜åˆ†æã€‚
- `plots/`ï¼ˆè‹¥è¿è¡Œ `scripts/run_analysis_demo.py`ï¼‰ï¼šè‡ªåŠ¨ç”Ÿæˆæ¼‚ç§»ã€ç­”æ¡ˆåˆ‡æ¢ã€ç½®ä¿¡åº¦ã€ä»ä¼—ç‡ç­‰ PNG å›¾è¡¨ã€‚

å½“æ”»å‡»è€…æœªèƒ½ç»´æŒæŒ‡å®šçš„é”™è¯¯ç­”æ¡ˆæ—¶ï¼Œè¯¥é—®é¢˜ä¼šè‡ªåŠ¨æ ‡è®°ä¸º `excluded: true`ï¼ŒåŒæ—¶è¢«å‰”é™¤å‡ºæ•´ä½“å‡†ç¡®ç‡å’Œä»ä¼—æŒ‡æ ‡çš„ç»Ÿè®¡èŒƒå›´ï¼Œç¡®ä¿åªå¯¹â€œæœ‰æ•ˆæ”»å‡»â€è¿›è¡Œåˆ†æã€‚
> Note: Drift and conformity metrics automatically ignore attacker/follower agents and only count conversions where a benign agent initially differed from the attacker but later matched the attackerâ€™s target answer (unless you switch the reference to ground truth).
